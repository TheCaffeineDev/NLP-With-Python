{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPACY FEATURES\n",
    "\n",
    "- ***Tokenization :***Segmenting text into words, punctuations marks etc. * Part-of-speech (POS) Tagging Assigning word types to tokens, like verb or noun.\n",
    "- Dependency Parsing Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.\n",
    "- ***Lemmatization :*** Assigning the base forms of words. For example, the lemma of \"was\" is \"be\", and the lemma of \"rats\" is \"rat\".\n",
    "- ***Sentence Boundary Detection (SBD):*** Finding and segmenting individual sentences.\n",
    "- ***Named Entity Recognition (NER):*** Labelling named \"real-world\" objects, like persons, companies or locations.\n",
    "- Similarity Comparing words, text spans and documents and how similar they are to each other.\n",
    "- ***Text Classification:*** Assigning categories or labels to a whole document, or parts of a document.\n",
    "- ***Rule-based Matching:*** Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.\n",
    "- 1Training Updating and improving a statistical model's predictions.\n",
    "- Serialization Saving objects to files or byte strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's really FAST\n",
    "Written in Cython, it was specifically designed to be as fast as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hello\"\n",
      "\"    \"\n",
      "\"World\"\n",
      "\"!\"\n"
     ]
    }
   ],
   "source": [
    "# start\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp('Hello     World!')\n",
    "for token in doc:\n",
    "    print('\"' + token.text + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the index preserving tokenization in action. Rather than only keeping the words, spaCy keeps the spaces too. This is helpful for situations when you need to replace words in the original text or add some annotations. With NLTK tokenization, there’s no way to know exactly where a tokenized word is in the original raw text. spaCy preserves this “link” between the word and its place in the raw text. Here’s how to get the exact index of a word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hello\" 0\n",
      "\"    \" 6\n",
      "\"World\" 10\n",
      "\"!\" 15\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp('Hello     World!')\n",
    "for token in doc:\n",
    "    print('\"' + token.text + '\"', token.idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Token class exposes a lot of word-level attributes. Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next\t0\tnext\tFalse\tFalse\tXxxx\tADJ\tJJ\n",
      "week\t5\tweek\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "I\t10\t-PRON-\tFalse\tFalse\tX\tPRON\tPRP\n",
      "'ll\t11\twill\tFalse\tFalse\t'xx\tVERB\tMD\n",
      "  \t15\t  \tFalse\tTrue\t  \tSPACE\tSP\n",
      "be\t17\tbe\tFalse\tFalse\txx\tVERB\tVB\n",
      "in\t20\tin\tFalse\tFalse\txx\tADP\tIN\n",
      "France\t23\tfrance\tFalse\tFalse\tXxxxx\tPROPN\tNNP\n",
      ".\t29\t.\tTrue\tFalse\t.\tPUNCT\t.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Next week I'll   be in France.\")\n",
    "for token in doc:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "        token.pos_,\n",
    "        token.tag_\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The spaCy toolbox\n",
    "\n",
    "Let’s now explore what are the models bundled up inside spaCy.\n",
    "\n",
    "### Sentence detection\n",
    "Here’s how to achieve one of the most common NLP tasks with spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are apples.\n",
      "These are oranges.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"These are apples. These are oranges.\")\n",
    " \n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Of Speech Tagging\n",
    "We’ve already seen how this works but let’s have another look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Next', 'JJ'), ('week', 'NN'), ('I', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('in', 'IN'), ('India', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(\"Next week I'll be in India.\")\n",
    "print([(token.text, token.tag_) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "Doing NER with spaCy is super easy and the pretrained model performs pretty well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next week DATE\n",
      "India GPE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Next week, I'll be in India.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also view the IOB style tagging of the sentence like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Next', 'JJ', 'O'), ('week', 'NN', 'O'), ('I', 'PRP', 'O'), (\"'ll\", 'MD', 'O'), ('be', 'VB', 'O'), ('in', 'IN', 'O'), ('Madrid', 'NNP', 'B-GPE'), ('.', '.', 'O')]\n",
      "(S Next/JJ week/NN I/PRP 'll/MD be/VB in/IN (GPE Madrid/NNP) ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import conlltags2tree\n",
    " \n",
    " \n",
    "doc = nlp(\"Next week I'll be in Madrid.\")\n",
    "iob_tagged = [\n",
    "    (\n",
    "        token.text, \n",
    "        token.tag_, \n",
    "        \"{0}-{1}\".format(token.ent_iob_, token.ent_type_) if token.ent_iob_ != 'O' else token.ent_iob_\n",
    "    ) for token in doc\n",
    "]\n",
    " \n",
    "print(iob_tagged)\n",
    " \n",
    "# In case you like the nltk.Tree format\n",
    "print(conlltags2tree(iob_tagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enitity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 CARDINAL\n",
      "9 a.m. because the stock went up 30% in just 2 days according to the WSJ TIME\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "spaCy automatically detects noun-phrases as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall Street Journal NP Journal\n",
      "an interesting piece NP piece\n",
      "crypto currencies NP currencies\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(\"Wall Street Journal just published an interesting piece on crypto currencies\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.label_, chunk.root.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing\n",
    "This is what makes spaCy really stand out. Let’s see the dependency parser in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall/NNP <--compound-- Journal/NNP\n",
      "Street/NNP <--compound-- Journal/NNP\n",
      "Journal/NNP <--nsubj-- published/VBD\n",
      "just/RB <--advmod-- published/VBD\n",
      "published/VBD <--ROOT-- published/VBD\n",
      "an/DT <--det-- piece/NN\n",
      "interesting/JJ <--amod-- piece/NN\n",
      "piece/NN <--dobj-- published/VBD\n",
      "on/IN <--prep-- piece/NN\n",
      "crypto/JJ <--amod-- currencies/NNS\n",
      "currencies/NNS <--pobj-- on/IN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
    " \n",
    "for token in doc:\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
    "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
